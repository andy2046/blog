# Persistent volumes
## Volumes
基本的Kubernetes存储抽象是卷。容器挂载绑定到其pod的卷，并且它们可以访问存储，就像它在本地文件系统中一样。
#### Using emptyDir for intra-pod communication
使用共享卷在同一个pod中的容器之间共享数据非常简单。容器1和容器2只需挂载相同的卷并可通过读写这个共享空间进行通信。最基本的volume是emptyDir。emptyDir卷是主机上的空目录。请注意，它不是持久的，因为当从节点中删除pod时，内容将被删除。如果一个容器挂了，pod还在，你可以稍后访问它的内容。另一个非常有趣的选择是使用RAM磁盘，将介质指定为Memory。现在，你的容器通过共享内存进行通信，这当然更快但不持久。如果节点重新启动，emptyDir的卷内容将丢失。以下是一个pod配置文件，它有两个容器，它们挂载了称为共享卷的相同卷。容器将它安装在不同的路径中，但是当hue-global-listener容器正在向/notifications发送文件时，hue-job-scheduler将在/incoming下看到该文件：
```YAML
apiVersion: v1
kind: Pod
metadata:
  name: hue-scheduler
spec:
  containers:
  - image: the_g1g1/hue-global-listener
    name: hue-global-listener
    volumeMounts:
    - mountPath: /notifications
      name: shared-volume
  - image: the_g1g1/hue-job-scheduler
    name: hue-job-scheduler
    volumeMounts:
    - mountPath: /incoming
      name: shared-volume
  volumes:
  - name: shared-volume
    emptyDir: {}
```
要使用共享内存选项，我们只需将medium：Memory添加到emptyDir部分：
```
volumes:
- name: shared-volume
  emptyDir:
    medium: Memory
```
#### Using HostPath for intra-node communication
有时，你希望Pod可以访问某些主机信息（例如，Docker守护进程），或者希望同一节点上的Pod能够相互通信。如果pod知道它们在同一主机上，这非常有用。由于Kubernetes基于可用资源安排Pod，Pod通常不知道它们共享节点的其他Pod。有两种情况，一个pod可以依赖于与其在同一节点上安排的其他pod：
* 在单节点集群中，所有集群显然共享同一个节点
* DaemonSet pod始终与任何其他与其选择器匹配的pod共享一个节点

例如，一个DeamonSet pod，它用作其他pod的代理。实现这种行为的另一种方式是，pod只需将其数据写入绑定到主机目录的已装入卷上，并且DaemonSet pod可以直接读取它并对其执行操作。

在决定使用HostPath卷之前，请确保你了解这些限制：
* 具有相同配置的pod，如果是数据驱动的并且其host上的文件不同，则行为可能会有所不同
* 它可能违反Kubernetes基于资源的调度，因为Kubernetes无法监控HostPath资源
* 访问主机目录的容器必须具有权限设置为true的安全上下文，或者在主机端，你需要更改权限以允许写入

下面是一个配置文件，它将/coupons目录挂载到映射到主机的/etc/hue/data/coupons目录的hue-coupon-hunter容器中：
```YAML
apiVersion: v1
kind: Pod
metadata:
  name: hue-coupon-hunter
spec:
  containers:
  - image: the_g1g1/hue-coupon-hunter
    name: hue-coupon-hunter
    volumeMounts:
    - mountPath: /coupons
      name: coupons-volume
  volumes:
  - name: coupons-volume
    host-path:
      path: /etc/hue/data/coupons
```
由于该pod没有安全上下文，因此无法写入主机目录。让我们通过添加一个安全上下文来改变容器规格来启用它：
```YAML
- image: the_g1g1/hue-coupon-hunter
  name: hue-coupon-hunter
  volumeMounts:
  - mountPath: /coupons
    name: coupons-volume
  securityContext:
    privileged: true
```
#### Provisioning persistent volumes
尽管emptyDir卷可以被容器装载和使用，但它们不是持久的，并且不需要任何特殊设置，因为它们使用节点上的现有存储。HostPath卷保留在原始节点上，但是如果pod在另一个节点上重新启动，它无法访问上一个节点的HostPath卷。真正的持久性卷使用管理员提前提供的存储。在云环境中，配置可能非常简单明了，但仍然是必需的，作为Kubernetes集群管理员，你至少必须确保存储配额充足，并认真监控用量与配额之间的关系。请记住，持久卷是类似于节点的Kubernetes集群使用的资源。因此，它们不由Kubernetes API服务器管理。你可以静态或动态调配资源。

静态配置非常简单。集群管理员提前创建由某些存储介质备份的持久性卷，这些持久性卷可以由容器声明。

当持久性卷声明与任何静态配置的持久卷不匹配时，可能会发生动态配置。如果声明指定了一个存储类，并且管理员将该类配置为动态配置，则可以动态配置持久性卷。
## Creating persistent volumes
下面是NFS持久卷的配置文件：
```YAML
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-1
  annotations:
    volume.beta.kubernetes.io/storage-class: "normal"
  labels:
    release: stable
    capacity: 100Gi
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteOnce
    - ReadOnlyMany
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /tmp
    server: 172.17.0.8
```
持久卷具有规范和元数据，其中包含存储类的名称和可能的注释。当存储类脱离测试版时，存储类注释将成为一个属性。请注意，持久卷位于v1，但存储类仍处于测试阶段。这个规范有四个部分：容量，访问模式，回收策略和卷类型（示例中的nfs）。
#### Capacity
每个卷都有一个指定的存储量。存储声明可以通过至少具有该存储量的持久性卷来满足。在该示例中，持久性卷具有100 Gibibytes（2 ^ 30字节）的容量。分配静态持久卷时了解存储请求模式非常重要。例如，如果你提供20个容量为100 GiB的持久性卷，一个容器声称一个容量为150 GiB的容器，那么即使总容量足够，这个声明也不会被满足。
#### Access modes
* ReadOnlyMany：可以被许多节点以只读方式挂载
* ReadWriteOnce：可以通过单个节点读写
* ReadWriteMany：可以作为多个节点的读写挂载

All access modes have a shorthand version:
* RWO – ReadWriteOnce
* ROX – ReadOnlyMany
* RWX – ReadWriteMany

存储挂载在节点上，因此即使使用ReadWriteOnce，同一节点上的多个容器也可以挂载卷并写入该卷。如果这会导致问题，则需要通过其他一些机制来处理它（例如，只在DaemonSet pods中声明卷，你知道每个节点只有一个pod）。
#### Reclaim policy
回收策略确定删除持久性卷声明时发生的情况。有三种不同的政策：
* 保留 - volume需要手动回收
* 删除 - 关联的存储，例如AWS EBS，GCE PD，Azure磁盘，或OpenStack Cinder卷被删除
* 回收 - 仅删除内容（rm -rf / volume / *）

保留和删除策略意味着持久卷不再适用于未来的声明。回收策略允许再次声明该卷。目前，只有NFS和HostPath支持回收。AWS EBS，GCE PD，Azure磁盘和Cinder卷支持删除。动态预配卷总是被删除。
#### Volume type
卷类型由规格中的名称指定。没有volumeType部分。在前面的例子中，nfs是卷的类型

## Making persistent volume claims
当容器需要访问某些持久性存储时，他们会提出声明。以下是与前一节中的持续性卷相匹配的示例性声明：
```YAML
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: storage-claim
  annotations:
    volume.beta.kubernetes.io/storage-class: "normal"
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 80Gi
  selector:
    matchLabels:
      release: "stable"
    matchExpressions:
      - {key: capacity, operator: In, values: [80Gi, 100Gi]}
```
在元数据中，你可以看到存储类注释。在将claim放入容器后，名称storage-claim很重要。规范中的访问模式为ReadWriteOnce，这意味着如果声明得到满足，则不会满足ReadWriteOnce访问模式的其他声明，但ReadOnlyMany的声明仍可以得到满足。资源部分请求80 GiB。这可以通过我们的容量为100 Gi的持续容量来满足。但这有点浪费，因为根据定义，20 Gi不会被使用。

选择器部分允许你进一步过滤可用卷。例如，此处的卷必须与标签版本stable匹配，并且其容量为80 Gi或100 Gi。想象一下，我们有几个其他卷供应容量为200 Gi和500 Gi。当我们只需要80Gi时，我们不想要求500G的volume。Kubernetes始终尝试匹配能够满足要求的最小卷，但如果没有80 Gi或100 Gi卷，则标签将阻止分配200 Gi或500 Gi卷，而使用动态预配。claim不用名字来匹配。Kubernetes根据存储类，容量和标签完成匹配。

最后，持久性volume声明属于一个名称空间。将持久卷绑定到claim是排他性的。这意味着持久卷将被绑定到一个名称空间。即使访问模式为ReadOnlyMany或ReadWriteMany，装入持久性卷声明的所有容器必须来自该声明的名称空间。
## Mounting claims as volumes
我们已经提供了一个volume并进行了声明。是时候在容器中使用声称的存储了。首先，持续volume声明必须用作pod中的volume，然后pod中的容器可以像其他volume一样挂载它。以下是一个pod配置文件，指定我们之前创建的持久性卷声明（绑定到我们调配的NFS持久卷）：
```YAML
kind: Pod
apiVersion: v1
metadata:
  name: the-pod
spec:
  containers:
    - name: the-container
      image: some-image
      volumeMounts:
      - mountPath: "/mnt/data"
        name: persistent-volume
  volumes:
    - name: persistent-volume
      persistentVolumeClaim:
        claimName: storage-claim
```
关键在volumes下的persistentVolumeClaim部分。声明名称（storage-claim）在当前名称空间中唯一标识特定声明，并在此处将其作为名为persistent-volume的卷提供。然后，容器可以通过其名称引用它并将其挂载到/mnt/data。
## Storage classes
通过存储类，管理员可以使用自定义持久性存储来配置集群（只要有适当的插件来支持它）。存储类在元数据中有一个名称（它必须在要注释中指定），供应商，和参数。
```YAML
kind: StorageClass
apiVersion: storage.k8s.io/v1beta1
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
```
当前支持的卷类型如下:
* emptyDir
* hostPath
* gcePersistentDisk
* awsElasticBlockStore
* nfs
* iscsi
* flocker
* glusterfs
* rbd
* cephfs
* gitRepo
* secret
* persistentVolumeClaim
* downwardAPI
* azureFileVolume
* azureDisk
* vsphereVolume
* Quobyte

#### Default storage class
集群管理员也可以分配一个默认存储类。如果分配了默认存储类并启用了DefaultStorageClass准入插件，那么将使用默认存储类动态分配没有存储类的声明。如果默认存储类不定义或者入口插件未打开，那么没有存储类的声明只能匹配没有存储类的卷。
# Public storage volume types
## AWS Elastic Block Store (EBS)
AWS提供弹性块存储作为EC2实例的持久性存储。AWS Kubernetes集群可以将AWS EBS用作持久存储，但有以下限制：
* pods必须作为节点在AWS EC2实例上运行
* pods只能访问其可用区域中配置的EBS卷
* EBS卷可以安装在单个EC2实例上

这些都是严重的限制。对单个可用区域的限制虽然对性能有好处，但却无法在规模上或跨地理分布式系统共享存储，而无需自定义复制和同步。单个EBS卷对单个EC2实例的限制意味着即使在相同的可用区域内，除非确保它们在同一节点上运行，否则pods不能共享存储（即使是读取）。
```YAML
apiVersion: v1
kind: Pod
metadata:
  name: some-pod
spec:
  containers:
  - image: some-container
    name: some-container
    volumeMounts:
    - mountPath: /ebs
      name: some-volume
  volumes:
  - name: some-volume
    awsElasticBlockStore:
      volumeID: <volume-id>
      fsType: ext4
```
你必须在AWS中创建EBS卷，然后将其挂载到该容器中。由于你直接按ID挂载卷，因此不需要claim或存储类。Kubernetes知道awsElasticBlockStore卷的类型。
## AWS Elastic File System (EFS)
AWS发布了一项名为弹性文件系统的服务。这实际上是一个托管的NFS服务。它使用NFS 4.1协议，与EBS相比有许多优点：
* 多个EC2实例可以跨多个可用区域AZ访问相同的文件（但在同一region内）
* 容量根据实际使用情况自动放大和缩小
* 你仅支付你使用的费用
* 你可以通过VPN将本地服务器连接到EFS
* EFS运行在可用区域自动复制的SSD驱动器
```YAML
apiVersion: v1
kind: PersistentVolume
metadata:
  name: efs-share
spec:
  capacity:
    storage: 200Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: eu-west-1b.fs-64HJku4i.efs.eu-west-1.amazonaws.com
    path: "/"
```
一旦持久卷存在，你可以为其创建声明，将声明作为卷附加到多个Pod（ReadWriteMany访问模式），并将其装入容器。
## GCE persistent disk
gcePersistentDisk卷类型与awsElasticBlockStore非常相似。你必须提前配置磁盘。它只能由GCE实例在同一个项目和区域中使用。但同一卷可以在多个实例上以只读方式使用。这意味着它支持ReadWriteOnce和ReadOnlyMany。你可以使用GCE永久磁盘在同一区域中的多个pod之间以只读方式共享数据。

在ReadWriteOnce模式下使用永久磁盘的pod必须由副本计数为0或1的复制控制器，副本集或部署控制，尝试缩放到1以外的操作将失败。
```YAML
apiVersion: v1
kind: Pod
metadata:
  name: some-pod
spec:
  containers:
  - image: some-container
    name: some-container
    volumeMounts:
    - mountPath: /pd
      name: some-volume
  volumes:
  - name: some-volume
    gcePersistentDisk:
      pdName: <persistent disk name>
      fsType: ext4
```
## Azure data disk
Azure数据磁盘是存储在Azure存储中的虚拟硬盘。它与AWS EBS的功能类似。以下是一个示例pod配置文件：
```YAML
apiVersion: v1
kind: Pod
metadata:
  name: some-pod
spec:
  containers:
  - image: some-container
    name: some-container
    volumeMounts:
      - name: some-volume
        mountPath: /azure
  volumes:
    - name: some-volume
      azureDisk:
        diskName: test.vhd
        diskURI: https://someaccount.blob.microsoft.net/vhds/test.vhd
```
除了必需的diskName和diskURI参数之外，它还有一些可选参数：
* cachingMode：磁盘缓存模式。这必须是None，ReadOnly或ReadWrite之一。默认值是None。
* fsType：设置为挂载的文件系统类型。默认值是ext4。
* readOnly：文件系统是否被用作只读。默认值是false。

Azure数据磁盘限制为1,023 GB。每个Azure虚拟机最多可以有16个数据磁盘。你可以将Azure数据磁盘附加到单个Azure VM。
## Azure  le storage
除了数据磁盘之外，Azure还有一个类似于AWS EFS的共享文件系统。但是，Azure文件存储使用SMB/CIFS协议（它支持SMB 2.1和SMB 3.0）。它基于Azure存储平台，具有与Azure Blob，Table或Queue相同的可用性，持久性，可扩展性和地理冗余功能。

为了使用Azure文件存储，你需要在每个客户端VM上安装cifs-utils软件包。你还需要创建一个secret，这是一个必需的参数：
```YAML
apiVersion: v1
kind: Secret
metadata:
  name: azure-file-secret
type: Opaque
data:
  azurestorageaccountname: <base64 encoded account name>
  azurestorageaccountkey: <base64 encoded account key>
```
```YAML
apiVersion: v1
kind: Pod
metadata:
 name: some-pod
spec:
 containers:
  - image: some-container
    name: some-container
    volumeMounts:
      - name: some-volume
        mountPath: /azure
 volumes:
      - name: some-volume
        azureFile:
          secretName: azure-file-secret
        shareName: azure-share
          readOnly: false
```
Azure文件存储支持在同一区域内共享以及连接本地客户端。
# GlusterFS and Ceph volumes in Kubernetes
GlusterFS和Ceph是两个分布式持久存储系统。GlusterFS的核心是一个网络系统。Ceph的核心是一个对象存储。都有块，对象和文件系统接口。两者均使用xfs文件系统将数据和元数据存储为xattr属性。有几个原因可能会导致你在Kubernetes集群中使用GlusterFs或Ceph作为持久卷：
* 你可能有许多数据和应用程序访问GlusterFS或Ceph中的数据
* 你拥有GlusterFS或Ceph的管理和运营专业知识
* 你在云中运行，但云平台持久存储🈶️限制
# Flocker
Flocker旨在让容器在节点之间移动时让Docker数据卷与其容器一起传输。如果你要将基于Docker的系统（如Docker compose或Mesos）迁移到Kubernetes，并使用Flocker编排存储，则可能需要使用Flocker卷插件。

Flocker在每个节点上都有控制服务和代理。其架构与Kubernetes的API服务器和Kubelet在每个节点上运行非常相似。Flocker控制服务暴露REST API并管理整个集群状态的配置。代理负责确保其节点的状态与当前配置相匹配。例如，如果数据集需要位于节点X上，则节点X上的Flocker代理将创建它。
# Integrating enterprise storage into Kubernetes
如果你有通过iSCSI接口暴露的现有存储区域网络（SAN），则Kubernetes为你提供一个卷插件。它与其他共享持久存储插件遵循相同的模型。你必须配置iSCSI启动器，但不必提供任何启动器信息。你需要提供的内容如下：
* iSCSI目标的IP地址和端口（如果不是默认的3260）
* Target的iqn（iSCSI限定名称） - 通常是颠倒的域名
* LUN - 逻辑单元号
* 文件系统类型
* 只读布尔标志

iSCSI插件支持ReadWriteOnce和ReadonlyMany。请注意，目前你无法对设备进行分区。
